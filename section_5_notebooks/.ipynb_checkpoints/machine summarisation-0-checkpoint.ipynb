{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords #provides list of english stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PRINT VERSION!!!\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('reviews.csv')#,  nrows=1000)  #, nrows=100000 sep='\\t',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['Summary','Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text_length'] = train['Text'].str.count(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    568454.000000\n",
       "mean         81.005522\n",
       "std          80.807102\n",
       "min           2.000000\n",
       "25%          33.000000\n",
       "50%          57.000000\n",
       "75%          99.000000\n",
       "max        3525.000000\n",
       "Name: text_length, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    568427.000000\n",
       "mean          3.128462\n",
       "std           2.619420\n",
       "min           0.000000\n",
       "25%           1.000000\n",
       "50%           3.000000\n",
       "75%           4.000000\n",
       "max          41.000000\n",
       "Name: summary_length, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['summary_length'] = train['Summary'].str.count(' ')\n",
    "train['summary_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[train['summary_length']<8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[train['text_length']<30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109053, 4)\n",
      "                   Summary                                               Text  \\\n",
      "4              Great taffy  Great taffy at a great price.  There was a wid...   \n",
      "7   Wonderful, tasty taffy  This taffy is so good.  It is very soft and ch...   \n",
      "8               Yay Barley  Right now I'm mostly just sprouting this so my...   \n",
      "9         Healthy Dog Food  This is a very healthy dog food. Good for thei...   \n",
      "13       fresh and greasy!  good flavor! these came securely packed... the...   \n",
      "\n",
      "    text_length  summary_length  \n",
      "4            29             1.0  \n",
      "7            27             2.0  \n",
      "8            25             1.0  \n",
      "9            24             2.0  \n",
      "13           14             2.0  \n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text_lower'] = train['Text'].str.lower()\n",
    "train['text_no_punctuation'] = train['text_lower'].str.replace('[^\\w\\s]','')\n",
    "#train['english_no_stopwords'] = train['english_no_punctuation'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#train[\"english_no_stopwords\"] = train[\"english_no_stopwords\"].fillna(\"fillna\")\n",
    "#train[\"english_no_stopwords\"] = train[\"english_no_stopwords\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['summary_lower'] = train[\"Summary\"].str.lower()\n",
    "train['summary_no_punctuation'] =  '_start_' + ' ' +train['summary_lower'].str.replace('[^\\w\\s]','')+ ' ' +'_end_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**VERY IMPORTANT TRICK!! NOTICE THAT WE ADD \"_start_\" and \"_end_\" EXACTLY AT THE BEGINNING AND THE END OF EACH SENTENCE TO HAVE SOME KIND OF'DELIMITERS' THAT WILL TELL OUR DECODER TO START AND FINISH. BECAUSE WE DON'T HAVE GENERAL SIGNALS OF START AND FINISH IN NATURAL LANGUAGE. BASICALLY '_end_' REFLECTS THE POINT IN WHICH OUR OUTPUT SENTENCE IS MORE LIKELY TO END.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features1 = 5000\n",
    "maxlen1 = 30\n",
    "\n",
    "max_features2 = 5000\n",
    "maxlen2 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok1 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features1) \n",
    "tok1.fit_on_texts(list(train['text_no_punctuation'].astype(str))) #fit to cleaned text\n",
    "tf_train_text =tok1.texts_to_sequences(list(train['text_no_punctuation'].astype(str)))\n",
    "tf_train_text =tf.keras.preprocessing.sequence.pad_sequences(tf_train_text, maxlen=maxlen1) #let's execute pad step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the processing has to be done for both \n",
    "#two different tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features2, filters = '*') \n",
    "tok2.fit_on_texts(list(train['summary_no_punctuation'].astype(str))) #fit to cleaned text\n",
    "tf_train_summary = tok2.texts_to_sequences(list(train['summary_no_punctuation'].astype(str)))\n",
    "tf_train_summary = tf.keras.preprocessing.sequence.pad_sequences(tf_train_summary, maxlen=maxlen2, padding ='post') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of decoder input: (109053, 7)\n",
      "Shape of decoder target: (109053, 7)\n",
      "Shape of encoder input: (109053, 30)\n"
     ]
    }
   ],
   "source": [
    "vectorized_summary = tf_train_summary\n",
    "# For Decoder Input, you don't need the last word as that is only for prediction\n",
    "# when we are training using Teacher Forcing.\n",
    "decoder_input_data = vectorized_summary[:, :-1]\n",
    "\n",
    "# Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n",
    "decoder_target_data = vectorized_summary[:, 1:]\n",
    "\n",
    "print(f'Shape of decoder input: {decoder_input_data.shape}')\n",
    "print(f'Shape of decoder target: {decoder_target_data.shape}')\n",
    "\n",
    "vectorized_text = tf_train_text\n",
    "# Encoder input is simply the body of the issue text\n",
    "encoder_input_data = vectorized_text\n",
    "doc_length = encoder_input_data.shape[1]\n",
    "print(f'Shape of encoder input: {encoder_input_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_encoder = len(tok1.word_index) + 1 #remember vocab size?\n",
    "vocab_size_decoder = len(tok2.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (English text)\n",
    "x = tf.keras.layers.Embedding(vocab_size_encoder, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "\n",
    "\n",
    "#Batch normalization is used so that the distribution of the inputs \n",
    "#to a specific layer doesn't change over time\n",
    "x = tf.keras.layers.BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = tf.keras.layers.GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = tf.keras.Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = tf.keras.Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (Italian text)\n",
    "dec_emb = tf.keras.layers.Embedding(vocab_size_decoder, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "#again batch normalization\n",
    "dec_bn = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = tf.keras.layers.GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out) #the decoder \"decodes\" the encoder output.\n",
    "x = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = tf.keras.layers.Dense(vocab_size_decoder, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "seq2seq_Model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "seq2seq_Model.compile(optimizer=tf.keras.optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Examine Model Architecture Summary **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 50)     692150      Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 50)     200         Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 50)           2013300     Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 50), ( 15300       Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 50)     200         Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 13843)  705993      Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 3,427,143\n",
      "Trainable params: 3,426,843\n",
      "Non-trainable params: 300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#from seq2seq_utils import viz_model_architecture\n",
    "seq2seq_Model.summary()\n",
    "#viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95966 samples, validate on 13087 samples\n",
      "Epoch 1/2\n",
      "95966/95966 [==============================] - 277s 3ms/sample - loss: 2.6403 - val_loss: 1.9723\n",
      "Epoch 2/2\n",
      "95966/95966 [==============================] - 269s 3ms/sample - loss: 1.7859 - val_loss: 1.7417\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 2 \n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,  epochs=epochs ,  validation_split=0.12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_text = ['apparently they used too much synthetic flavors that it just burns your tongue also theres too much oil  almost made me chok']\n",
    "#test_text = ['this stuff is awesome  for best flavor boil it in water drain the water add spice packet and then add hot water']\n",
    "test_text = ['this product is great  gives you so much energy and tastes great  try this cafe latte and all the other flavors and you will not be disappointed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq2seq_Model = tf.keras.models.load_model('seq2seq_subsample_1_epochs.h5')\n",
    "#seq2seq_Model = tf.keras.models.load_model('seq2seq_full_data_3_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_len_title = 30\n",
    "# get the encoder's features for the decoder\n",
    "tok1.fit_on_texts(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tokenized = tok1.texts_to_sequences(test_text)\n",
    "raw_tokenized = tf.keras.preprocessing.sequence.pad_sequences(raw_tokenized, maxlen=maxlen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_encoding = encoder_model.predict(raw_tokenized) #predict the encoder state of the new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = seq2seq_Model.get_layer('Decoder-Word-Embedding').output_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember the get layer methodo for getting the embedding (word clusters)\n",
    "decoder_inputs = seq2seq_Model.get_layer('Decoder-Input').input \n",
    "dec_emb = seq2seq_Model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
    "dec_bn = seq2seq_Model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_inference_state_input = tf.keras.Input(shape=(latent_dim,), name='hidden_state_input')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_out, gru_state_out = seq2seq_Model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct dense layers\n",
    "dec_bn2 = seq2seq_Model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
    "dense_out = seq2seq_Model.get_layer('Final-Output-Dense')(dec_bn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = tf.keras.Model([decoder_inputs, gru_inference_state_input],\n",
    "                          [dense_out, gru_state_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to save the encoder's embedding before its updated by decoder\n",
    "#   because we can use that as an embedding for other tasks.\n",
    "original_body_encoding = body_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value = np.array(tok2.word_index['_start_']).reshape(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentence = []\n",
    "stop_condition = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_inv = dict((v, k) for k, v in tok2.word_index.items())\n",
    "#vocabulary_inv[0] = \"<PAD/>\"\n",
    "#vocabulary_inv[1] = \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '_start_',\n",
       " 2: '_end_',\n",
       " 3: 'great',\n",
       " 4: 'good',\n",
       " 5: 'the',\n",
       " 6: 'coffee',\n",
       " 7: 'best',\n",
       " 8: 'love',\n",
       " 9: 'tea',\n",
       " 10: 'for',\n",
       " 11: 'product',\n",
       " 12: 'and',\n",
       " 13: 'it',\n",
       " 14: 'delicious',\n",
       " 15: 'my',\n",
       " 16: 'a',\n",
       " 17: 'not',\n",
       " 18: 'this',\n",
       " 19: 'taste',\n",
       " 20: 'yummy',\n",
       " 21: 'excellent',\n",
       " 22: 'very',\n",
       " 23: 'dog',\n",
       " 24: 'price',\n",
       " 25: 'flavor',\n",
       " 26: 'i',\n",
       " 27: 'of',\n",
       " 28: 'is',\n",
       " 29: 'to',\n",
       " 30: 'these',\n",
       " 31: 'tasty',\n",
       " 32: 'stuff',\n",
       " 33: 'but',\n",
       " 34: 'ever',\n",
       " 35: 'favorite',\n",
       " 36: 'yum',\n",
       " 37: 'like',\n",
       " 38: 'snack',\n",
       " 39: 'food',\n",
       " 40: 'awesome',\n",
       " 41: 'loves',\n",
       " 42: 'in',\n",
       " 43: 'too',\n",
       " 44: 'chocolate',\n",
       " 45: 'wonderful',\n",
       " 46: 'dogs',\n",
       " 47: 'chips',\n",
       " 48: 'as',\n",
       " 49: 'so',\n",
       " 50: 'them',\n",
       " 51: 'treats',\n",
       " 52: 'just',\n",
       " 53: 'are',\n",
       " 54: 'nice',\n",
       " 55: 'hot',\n",
       " 56: 'healthy',\n",
       " 57: 'free',\n",
       " 58: 'with',\n",
       " 59: 'tasting',\n",
       " 60: 'cookies',\n",
       " 61: 'buy',\n",
       " 62: 'treat',\n",
       " 63: 'quality',\n",
       " 64: 'tastes',\n",
       " 65: 'perfect',\n",
       " 66: 'better',\n",
       " 67: 'you',\n",
       " 68: 'on',\n",
       " 69: 'sweet',\n",
       " 70: 'value',\n",
       " 71: 'at',\n",
       " 72: 'green',\n",
       " 73: 'really',\n",
       " 74: 'what',\n",
       " 75: 'no',\n",
       " 76: 'candy',\n",
       " 77: 'its',\n",
       " 78: 'amazing',\n",
       " 79: 'than',\n",
       " 80: 'mix',\n",
       " 81: 'organic',\n",
       " 82: 'cats',\n",
       " 83: 'deal',\n",
       " 84: 'sugar',\n",
       " 85: 'cup',\n",
       " 86: 'gluten',\n",
       " 87: 'cat',\n",
       " 88: 'one',\n",
       " 89: 'service',\n",
       " 90: 'easy',\n",
       " 91: 'ok',\n",
       " 92: 'popcorn',\n",
       " 93: 'bad',\n",
       " 94: 'salt',\n",
       " 95: 'all',\n",
       " 96: 'little',\n",
       " 97: 'bars',\n",
       " 98: 'decaf',\n",
       " 99: 'kcups',\n",
       " 100: 'me',\n",
       " 101: 'gift',\n",
       " 102: 'fantastic',\n",
       " 103: 'review',\n",
       " 104: 'breakfast',\n",
       " 105: 'happy',\n",
       " 106: 'cereal',\n",
       " 107: 'strong',\n",
       " 108: 'oil',\n",
       " 109: 'kcup',\n",
       " 110: 'sauce',\n",
       " 111: 'fresh',\n",
       " 112: 'dont',\n",
       " 113: 'works',\n",
       " 114: 'from',\n",
       " 115: 'fast',\n",
       " 116: 'k',\n",
       " 117: 'expensive',\n",
       " 118: 'loved',\n",
       " 119: 'vanilla',\n",
       " 120: 'dark',\n",
       " 121: 'much',\n",
       " 122: 'coconut',\n",
       " 123: 'blend',\n",
       " 124: 'bold',\n",
       " 125: 'bar',\n",
       " 126: 'way',\n",
       " 127: 'rice',\n",
       " 128: 'super',\n",
       " 129: 'drink',\n",
       " 130: 'have',\n",
       " 131: 'time',\n",
       " 132: 'they',\n",
       " 133: 'pretty',\n",
       " 134: 'cant',\n",
       " 135: 'jerky',\n",
       " 136: 'your',\n",
       " 137: 'gum',\n",
       " 138: 'soup',\n",
       " 139: 'was',\n",
       " 140: 'more',\n",
       " 141: 'be',\n",
       " 142: 'hard',\n",
       " 143: 'real',\n",
       " 144: 'butter',\n",
       " 145: 'amazon',\n",
       " 146: 'low',\n",
       " 147: 'cups',\n",
       " 148: 'go',\n",
       " 149: 'find',\n",
       " 150: 'natural',\n",
       " 151: 'up',\n",
       " 152: 'pasta',\n",
       " 153: 'chai',\n",
       " 154: 'kids',\n",
       " 155: 'smooth',\n",
       " 156: 'baby',\n",
       " 157: 'syrup',\n",
       " 158: 'our',\n",
       " 159: 'nuts',\n",
       " 160: 'kind',\n",
       " 161: 'only',\n",
       " 162: 'fabulous',\n",
       " 163: 'out',\n",
       " 164: 'addictive',\n",
       " 165: 'do',\n",
       " 166: 'item',\n",
       " 167: 'high',\n",
       " 168: 'protein',\n",
       " 169: 'mountain',\n",
       " 170: 'chicken',\n",
       " 171: 'expected',\n",
       " 172: 'beans',\n",
       " 173: 'wow',\n",
       " 174: 'old',\n",
       " 175: 'big',\n",
       " 176: 'honey',\n",
       " 177: 'water',\n",
       " 178: 'bread',\n",
       " 179: 'quick',\n",
       " 180: 'cocoa',\n",
       " 181: 'greenies',\n",
       " 182: 'pretzels',\n",
       " 183: 'shipping',\n",
       " 184: 'spicy',\n",
       " 185: 'cookie',\n",
       " 186: 'had',\n",
       " 187: 'roast',\n",
       " 188: 'ginger',\n",
       " 189: 'disappointed',\n",
       " 190: 'cheaper',\n",
       " 191: 'worth',\n",
       " 192: 'ive',\n",
       " 193: 'that',\n",
       " 194: 'crackers',\n",
       " 195: 'again',\n",
       " 196: 'keurig',\n",
       " 197: 'made',\n",
       " 198: 'get',\n",
       " 199: 'money',\n",
       " 200: 'em',\n",
       " 201: 'milk',\n",
       " 202: 'alternative',\n",
       " 203: 'snacks',\n",
       " 204: 'hazelnut',\n",
       " 205: 'black',\n",
       " 206: 'peanut',\n",
       " 207: 'yuck',\n",
       " 208: 'stale',\n",
       " 209: 'delivery',\n",
       " 210: 'chews',\n",
       " 211: 'stash',\n",
       " 212: 'or',\n",
       " 213: 'overpriced',\n",
       " 214: 'will',\n",
       " 215: 'instant',\n",
       " 216: 'convenient',\n",
       " 217: 'spice',\n",
       " 218: 'seasoning',\n",
       " 219: 'salty',\n",
       " 220: 'there',\n",
       " 221: 'okay',\n",
       " 222: 'wrong',\n",
       " 223: 'fruit',\n",
       " 224: 'by',\n",
       " 225: 'morning',\n",
       " 226: 'energy',\n",
       " 227: 'cheese',\n",
       " 228: 'makes',\n",
       " 229: 'order',\n",
       " 230: 'potato',\n",
       " 231: 'new',\n",
       " 232: 'pods',\n",
       " 233: 'can',\n",
       " 234: 'oatmeal',\n",
       " 235: 'weak',\n",
       " 236: 'well',\n",
       " 237: 'light',\n",
       " 238: 'red',\n",
       " 239: 'bags',\n",
       " 240: 'small',\n",
       " 241: 'beef',\n",
       " 242: 'espresso',\n",
       " 243: 'pop',\n",
       " 244: 'an',\n",
       " 245: 'we',\n",
       " 246: 'horrible',\n",
       " 247: 'if',\n",
       " 248: 'white',\n",
       " 249: 'terrible',\n",
       " 250: 'licorice',\n",
       " 251: 'bitter',\n",
       " 252: 'chip',\n",
       " 253: 'right',\n",
       " 254: 'absolutely',\n",
       " 255: 'french',\n",
       " 256: 'eat',\n",
       " 257: 'doesnt',\n",
       " 258: 'lemon',\n",
       " 259: 'refreshing',\n",
       " 260: 'pack',\n",
       " 261: 'powder',\n",
       " 262: 'delish',\n",
       " 263: 'without',\n",
       " 264: 'use',\n",
       " 265: 'work',\n",
       " 266: 'crunchy',\n",
       " 267: 'always',\n",
       " 268: 'starbucks',\n",
       " 269: 'enough',\n",
       " 270: 'nothing',\n",
       " 271: 'simply',\n",
       " 272: 'around',\n",
       " 273: 'pleased',\n",
       " 274: 'dried',\n",
       " 275: 'almonds',\n",
       " 276: 'awful',\n",
       " 277: 'flavored',\n",
       " 278: 'senseo',\n",
       " 279: 'dry',\n",
       " 280: 'never',\n",
       " 281: 'whole',\n",
       " 282: 'didnt',\n",
       " 283: 'cinnamon',\n",
       " 284: 'packaging',\n",
       " 285: 'variety',\n",
       " 286: 'granola',\n",
       " 287: 'choice',\n",
       " 288: 'glutenfree',\n",
       " 289: 'bit',\n",
       " 290: 'fat',\n",
       " 291: 'does',\n",
       " 292: 'brand',\n",
       " 293: 'flavorful',\n",
       " 294: 'lover',\n",
       " 295: 'seeds',\n",
       " 296: 'fun',\n",
       " 297: 'purchase',\n",
       " 298: '5',\n",
       " 299: 'diet',\n",
       " 300: 'house',\n",
       " 301: 'rich',\n",
       " 302: 'juice',\n",
       " 303: 'size',\n",
       " 304: 'has',\n",
       " 305: 'lovers',\n",
       " 306: 'some',\n",
       " 307: 'cake',\n",
       " 308: 'cappuccino',\n",
       " 309: 'were',\n",
       " 310: 'another',\n",
       " 311: 'nasty',\n",
       " 312: 'customer',\n",
       " 313: 'special',\n",
       " 314: 'make',\n",
       " 315: 'outstanding',\n",
       " 316: 'mint',\n",
       " 317: 'blue',\n",
       " 318: 'flavors',\n",
       " 319: 'world',\n",
       " 320: 'did',\n",
       " 321: 'even',\n",
       " 322: 'nom',\n",
       " 323: 'regular',\n",
       " 324: 'smells',\n",
       " 325: 'nut',\n",
       " 326: 'found',\n",
       " 327: 'goodness',\n",
       " 328: 'box',\n",
       " 329: 'iced',\n",
       " 330: 'substitute',\n",
       " 331: 'maple',\n",
       " 332: 'training',\n",
       " 333: 'italian',\n",
       " 334: 'gross',\n",
       " 335: 'pumpkin',\n",
       " 336: 'puppy',\n",
       " 337: 'apple',\n",
       " 338: 'calm',\n",
       " 339: 'tasted',\n",
       " 340: 'sour',\n",
       " 341: 'thing',\n",
       " 342: 'puck',\n",
       " 343: 'market',\n",
       " 344: 'salmon',\n",
       " 345: 'disgusting',\n",
       " 346: 'over',\n",
       " 347: 'stevia',\n",
       " 348: 'pricey',\n",
       " 349: 'fine',\n",
       " 350: 'off',\n",
       " 351: 'raspberry',\n",
       " 352: 'noodles',\n",
       " 353: 'bones',\n",
       " 354: 'heaven',\n",
       " 355: 'teas',\n",
       " 356: 'decent',\n",
       " 357: 'herbal',\n",
       " 358: 'likes',\n",
       " 359: 'corn',\n",
       " 360: 'would',\n",
       " 361: 'far',\n",
       " 362: 'poor',\n",
       " 363: 'pepper',\n",
       " 364: 'why',\n",
       " 365: 'most',\n",
       " 366: 'hit',\n",
       " 367: 'son',\n",
       " 368: 'china',\n",
       " 369: 'crazy',\n",
       " 370: 'oh',\n",
       " 371: 'satisfied',\n",
       " 372: 'full',\n",
       " 373: 'date',\n",
       " 374: 'original',\n",
       " 375: 'jet',\n",
       " 376: 'every',\n",
       " 377: 'wheat',\n",
       " 378: 'pure',\n",
       " 379: 'fuel',\n",
       " 380: 'different',\n",
       " 381: 'bbq',\n",
       " 382: 'mild',\n",
       " 383: 'must',\n",
       " 384: 'ice',\n",
       " 385: 'biscuits',\n",
       " 386: 'long',\n",
       " 387: 'im',\n",
       " 388: 'other',\n",
       " 389: 'yet',\n",
       " 390: '2',\n",
       " 391: 'baking',\n",
       " 392: 'chew',\n",
       " 393: 'olive',\n",
       " 394: 'received',\n",
       " 395: 'terrific',\n",
       " 396: 'jelly',\n",
       " 397: 'dressing',\n",
       " 398: 'canned',\n",
       " 399: 'aroma',\n",
       " 400: 'earl',\n",
       " 401: 'priced',\n",
       " 402: 'orange',\n",
       " 403: 'bag',\n",
       " 404: 'extra',\n",
       " 405: 'flour',\n",
       " 406: 'grove',\n",
       " 407: 'almost',\n",
       " 408: 'magic',\n",
       " 409: 'got',\n",
       " 410: 'peppermint',\n",
       " 411: 'mustard',\n",
       " 412: 'gf',\n",
       " 413: 'less',\n",
       " 414: 'plus',\n",
       " 415: 'soft',\n",
       " 416: 'grey',\n",
       " 417: 'english',\n",
       " 418: 'exactly',\n",
       " 419: 'when',\n",
       " 420: 'cream',\n",
       " 421: 'wolfgang',\n",
       " 422: 'store',\n",
       " 423: 'finally',\n",
       " 424: 'still',\n",
       " 425: 'newmans',\n",
       " 426: 'crunch',\n",
       " 427: 'vinegar',\n",
       " 428: 'life',\n",
       " 429: 'chili',\n",
       " 430: 'premium',\n",
       " 431: 'here',\n",
       " 432: 'family',\n",
       " 433: 'popchips',\n",
       " 434: 'soy',\n",
       " 435: 'ordered',\n",
       " 436: 'products',\n",
       " 437: 'doggie',\n",
       " 438: 'liked',\n",
       " 439: 'try',\n",
       " 440: 'says',\n",
       " 441: 'mini',\n",
       " 442: 'wellness',\n",
       " 443: 'brew',\n",
       " 444: 'broken',\n",
       " 445: 'last',\n",
       " 446: 'day',\n",
       " 447: 'formula',\n",
       " 448: 'meal',\n",
       " 449: 'kitty',\n",
       " 450: 'package',\n",
       " 451: 'pill',\n",
       " 452: 'rocks',\n",
       " 453: 'say',\n",
       " 454: 'greatest',\n",
       " 455: 'about',\n",
       " 456: 'seed',\n",
       " 457: 'described',\n",
       " 458: '1',\n",
       " 459: 'mom',\n",
       " 460: 'how',\n",
       " 461: 'bland',\n",
       " 462: 'blueberry',\n",
       " 463: 'kona',\n",
       " 464: 'bacon',\n",
       " 465: 'back',\n",
       " 466: 'many',\n",
       " 467: 'picky',\n",
       " 468: 'jeans',\n",
       " 469: 'bean',\n",
       " 470: 'soda',\n",
       " 471: 'worst',\n",
       " 472: 'cheap',\n",
       " 473: 'same',\n",
       " 474: 'disappointing',\n",
       " 475: 'sea',\n",
       " 476: 'waste',\n",
       " 477: 'available',\n",
       " 478: 'garlic',\n",
       " 479: 'chewy',\n",
       " 480: 'lovely',\n",
       " 481: 'top',\n",
       " 482: 'yes',\n",
       " 483: 'kettle',\n",
       " 484: 'sticks',\n",
       " 485: 'first',\n",
       " 486: 'could',\n",
       " 487: 'wont',\n",
       " 488: 'grass',\n",
       " 489: 'stars',\n",
       " 490: 'cute',\n",
       " 491: 'company',\n",
       " 492: 'looking',\n",
       " 493: 'filling',\n",
       " 494: 'raw',\n",
       " 495: 'brown',\n",
       " 496: 'any',\n",
       " 497: 'square',\n",
       " 498: 'tried',\n",
       " 499: 'almond',\n",
       " 500: 'coffe',\n",
       " 501: 'pockets',\n",
       " 502: 'cooking',\n",
       " 503: 'beware',\n",
       " 504: 'extract',\n",
       " 505: 'mmmmm',\n",
       " 506: 'delight',\n",
       " 507: 'superb',\n",
       " 508: 'movie',\n",
       " 509: 'arrived',\n",
       " 510: 'simple',\n",
       " 511: 'caribou',\n",
       " 512: 'highly',\n",
       " 513: 'definitely',\n",
       " 514: 'chocolates',\n",
       " 515: 'cracker',\n",
       " 516: 'bought',\n",
       " 517: 'own',\n",
       " 518: 'helps',\n",
       " 519: 'cold',\n",
       " 520: 'caramel',\n",
       " 521: 'carb',\n",
       " 522: 'eating',\n",
       " 523: 'now',\n",
       " 524: 'gloria',\n",
       " 525: 'grain',\n",
       " 526: 'favorites',\n",
       " 527: 'nestle',\n",
       " 528: 'zukes',\n",
       " 529: 'add',\n",
       " 530: 'creamy',\n",
       " 531: 'gone',\n",
       " 532: 'cherry',\n",
       " 533: 'actually',\n",
       " 534: 'slim',\n",
       " 535: 'jam',\n",
       " 536: 'ingredients',\n",
       " 537: 'tuna',\n",
       " 538: 'nutritious',\n",
       " 539: 'cider',\n",
       " 540: 'theyre',\n",
       " 541: 'true',\n",
       " 542: 'peanuts',\n",
       " 543: 'advertised',\n",
       " 544: 'twinings',\n",
       " 545: 'gummy',\n",
       " 546: 'need',\n",
       " 547: 'home',\n",
       " 548: 'wild',\n",
       " 549: 'timothys',\n",
       " 550: 'used',\n",
       " 551: 'rose',\n",
       " 552: 'lot',\n",
       " 553: 'mints',\n",
       " 554: 'calorie',\n",
       " 555: 'everything',\n",
       " 556: '100',\n",
       " 557: 'yummm',\n",
       " 558: 'salad',\n",
       " 559: 'vegan',\n",
       " 560: 'after',\n",
       " 561: 'rip',\n",
       " 562: 'large',\n",
       " 563: 'think',\n",
       " 564: 'toy',\n",
       " 565: 'sodium',\n",
       " 566: 'wine',\n",
       " 567: 'totally',\n",
       " 568: 'chamomile',\n",
       " 569: 'truffle',\n",
       " 570: 'pet',\n",
       " 571: 'gourmet',\n",
       " 572: 'wife',\n",
       " 573: 'bargain',\n",
       " 574: 'bears',\n",
       " 575: 'cherries',\n",
       " 576: 'idea',\n",
       " 577: 'down',\n",
       " 578: 'diamond',\n",
       " 579: 'lipton',\n",
       " 580: 'earth',\n",
       " 581: 'meh',\n",
       " 582: 'lots',\n",
       " 583: 'anywhere',\n",
       " 584: 'teeth',\n",
       " 585: 'yumm',\n",
       " 586: 'strawberry',\n",
       " 587: 'berry',\n",
       " 588: 'name',\n",
       " 589: 'tiny',\n",
       " 590: 'mocha',\n",
       " 591: 'please',\n",
       " 592: 'recommend',\n",
       " 593: 'quite',\n",
       " 594: 'option',\n",
       " 595: 'mmm',\n",
       " 596: 'latte',\n",
       " 597: 'people',\n",
       " 598: 'mmmm',\n",
       " 599: 'fan',\n",
       " 600: 'addicted',\n",
       " 601: 'homemade',\n",
       " 602: 'save',\n",
       " 603: 'bed',\n",
       " 604: 'satisfying',\n",
       " 605: 'wafers',\n",
       " 606: 'robust',\n",
       " 607: 'clean',\n",
       " 608: 'pecan',\n",
       " 609: 'thanks',\n",
       " 610: 'lime',\n",
       " 611: 'stores',\n",
       " 612: 'star',\n",
       " 613: 'close',\n",
       " 614: 'meat',\n",
       " 615: 'crystal',\n",
       " 616: 'fiber',\n",
       " 617: 'power',\n",
       " 618: 'awsome',\n",
       " 619: 'basket',\n",
       " 620: 'average',\n",
       " 621: 'wish',\n",
       " 622: 'pod',\n",
       " 623: 'smell',\n",
       " 624: 'classic',\n",
       " 625: 'bulk',\n",
       " 626: 'hip',\n",
       " 627: 'peach',\n",
       " 628: 'splenda',\n",
       " 629: 'things',\n",
       " 630: 'lavazza',\n",
       " 631: 'seller',\n",
       " 632: 'fix',\n",
       " 633: 'pie',\n",
       " 634: 'addicting',\n",
       " 635: 'medium',\n",
       " 636: 'beautiful',\n",
       " 637: 'toffee',\n",
       " 638: 'joe',\n",
       " 639: 'omg',\n",
       " 640: 'handy',\n",
       " 641: 'husband',\n",
       " 642: 'christmas',\n",
       " 643: 'steak',\n",
       " 644: 'texture',\n",
       " 645: 'aftertaste',\n",
       " 646: '4',\n",
       " 647: 'solid',\n",
       " 648: 'pizza',\n",
       " 649: 'golden',\n",
       " 650: 'should',\n",
       " 651: 'double',\n",
       " 652: 'health',\n",
       " 653: 'yuk',\n",
       " 654: 'paste',\n",
       " 655: 'mango',\n",
       " 656: 'ones',\n",
       " 657: 'dental',\n",
       " 658: 'cans',\n",
       " 659: 'emerils',\n",
       " 660: 'needed',\n",
       " 661: 'tullys',\n",
       " 662: 'hemp',\n",
       " 663: 'who',\n",
       " 664: 'enjoy',\n",
       " 665: 'banana',\n",
       " 666: 'live',\n",
       " 667: 'curry',\n",
       " 668: 'calories',\n",
       " 669: 'summer',\n",
       " 670: 'dessert',\n",
       " 671: 'cafe',\n",
       " 672: 'jack',\n",
       " 673: 'wanted',\n",
       " 674: 'thank',\n",
       " 675: 'delightful',\n",
       " 676: 'candies',\n",
       " 677: '3',\n",
       " 678: 'crack',\n",
       " 679: 'oz',\n",
       " 680: 'tomato',\n",
       " 681: 'tough',\n",
       " 682: 'feel',\n",
       " 683: 'party',\n",
       " 684: 'beer',\n",
       " 685: 'misleading',\n",
       " 686: 'creme',\n",
       " 687: 'everyday',\n",
       " 688: 'foods',\n",
       " 689: 'cola',\n",
       " 690: 'relaxing',\n",
       " 691: 'missing',\n",
       " 692: 'daughter',\n",
       " 693: 'olives',\n",
       " 694: 'bone',\n",
       " 695: 'mill',\n",
       " 696: 'start',\n",
       " 697: 'crisp',\n",
       " 698: 'half',\n",
       " 699: 'lunch',\n",
       " 700: 'tree',\n",
       " 701: 'mmmmmm',\n",
       " 702: 'cost',\n",
       " 703: 'ground',\n",
       " 704: 'pork',\n",
       " 705: 'bobs',\n",
       " 706: 'plant',\n",
       " 707: 'slow',\n",
       " 708: 'quaker',\n",
       " 709: 'stop',\n",
       " 710: 'bottle',\n",
       " 711: 'agave',\n",
       " 712: 'noodle',\n",
       " 713: 'country',\n",
       " 714: 'job',\n",
       " 715: 'expiration',\n",
       " 716: 'bite',\n",
       " 717: 'else',\n",
       " 718: 'five',\n",
       " 719: 'replacement',\n",
       " 720: 'puffs',\n",
       " 721: 'goes',\n",
       " 722: 'fish',\n",
       " 723: 'berries',\n",
       " 724: 'jasmine',\n",
       " 725: 'irish',\n",
       " 726: 'liver',\n",
       " 727: 'thought',\n",
       " 728: 'pancake',\n",
       " 729: 'plain',\n",
       " 730: 'shipment',\n",
       " 731: 'pb',\n",
       " 732: 'rawhide',\n",
       " 733: 'gummi',\n",
       " 734: 'flakes',\n",
       " 735: 'us',\n",
       " 736: 'sugarfree',\n",
       " 737: 'oats',\n",
       " 738: 'yumo',\n",
       " 739: 'authentic',\n",
       " 740: 'selection',\n",
       " 741: 'excelent',\n",
       " 742: 'give',\n",
       " 743: 'grocery',\n",
       " 744: 'bran',\n",
       " 745: 'horsetail',\n",
       " 746: 'kidding',\n",
       " 747: 'keeps',\n",
       " 748: 'dinner',\n",
       " 749: 'delicous',\n",
       " 750: 'everyone',\n",
       " 751: 'brownies',\n",
       " 752: 'artificial',\n",
       " 753: 'sweetener',\n",
       " 754: 'pink',\n",
       " 755: 'kid',\n",
       " 756: 'two',\n",
       " 757: 'wake',\n",
       " 758: 'needs',\n",
       " 759: 'rub',\n",
       " 760: 'pops',\n",
       " 761: 'sampler',\n",
       " 762: 'baked',\n",
       " 763: 'impressed',\n",
       " 764: 'where',\n",
       " 765: 'unique',\n",
       " 766: 'recommended',\n",
       " 767: 'assortment',\n",
       " 768: 'roasted',\n",
       " 769: 'source',\n",
       " 770: 'transaction',\n",
       " 771: 'kit',\n",
       " 772: 'gold',\n",
       " 773: 'donut',\n",
       " 774: 'fragrant',\n",
       " 775: 'rock',\n",
       " 776: 'sardines',\n",
       " 777: 'dr',\n",
       " 778: 'excellant',\n",
       " 779: '12',\n",
       " 780: 'sumatra',\n",
       " 781: 'crispy',\n",
       " 782: 'expired',\n",
       " 783: 'pamelas',\n",
       " 784: 'said',\n",
       " 785: 'turkey',\n",
       " 786: 'contains',\n",
       " 787: 'scrumptious',\n",
       " 788: 'yummmmm',\n",
       " 789: 'style',\n",
       " 790: 'naturals',\n",
       " 791: 'lamb',\n",
       " 792: 'nutrition',\n",
       " 793: 'sesame',\n",
       " 794: 'haribo',\n",
       " 795: 'yummmm',\n",
       " 796: 'nutiva',\n",
       " 797: 'disappointment',\n",
       " 798: 'thin',\n",
       " 799: 'flowers',\n",
       " 800: 'wholesome',\n",
       " 801: '10',\n",
       " 802: 'bites',\n",
       " 803: 'nature',\n",
       " 804: 'melted',\n",
       " 805: 'want',\n",
       " 806: 'something',\n",
       " 807: 'moist',\n",
       " 808: 'cet',\n",
       " 809: 'sweetner',\n",
       " 810: 'surprisingly',\n",
       " 811: 'gets',\n",
       " 812: 'tasteless',\n",
       " 813: 'pleasant',\n",
       " 814: 'interesting',\n",
       " 815: 'hawaiian',\n",
       " 816: 'wasnt',\n",
       " 817: 'jim',\n",
       " 818: 'am',\n",
       " 819: 'pb2',\n",
       " 820: 'bay',\n",
       " 821: 'elsewhere',\n",
       " 822: 'rum',\n",
       " 823: 'spices',\n",
       " 824: 'cool',\n",
       " 825: 'before',\n",
       " 826: 'care',\n",
       " 827: 'staple',\n",
       " 828: 'raisins',\n",
       " 829: 'experience',\n",
       " 830: 'lollipops',\n",
       " 831: 'swiss',\n",
       " 832: 'surprise',\n",
       " 833: 'weight',\n",
       " 834: 'waffles',\n",
       " 835: 'gotta',\n",
       " 836: 'coffees',\n",
       " 837: 'powdered',\n",
       " 838: 'buying',\n",
       " 839: 'sooo',\n",
       " 840: 'hour',\n",
       " 841: 'peggy',\n",
       " 842: 'recipe',\n",
       " 843: 'soooo',\n",
       " 844: 'away',\n",
       " 845: 'incredible',\n",
       " 846: 'caffeine',\n",
       " 847: 'altoids',\n",
       " 848: 'fair',\n",
       " 849: 'addition',\n",
       " 850: 'loose',\n",
       " 851: 'miss',\n",
       " 852: 'strips',\n",
       " 853: 'healthier',\n",
       " 854: 'mac',\n",
       " 855: 'hips',\n",
       " 856: 'freeze',\n",
       " 857: 'sent',\n",
       " 858: 'zico',\n",
       " 859: 'vitality',\n",
       " 860: 'taffy',\n",
       " 861: 'o',\n",
       " 862: 'costco',\n",
       " 863: 'pancakes',\n",
       " 864: 'san',\n",
       " 865: 'coco',\n",
       " 866: 'leaf',\n",
       " 867: 'n',\n",
       " 868: 'columbian',\n",
       " 869: 'yogi',\n",
       " 870: 'dissapointed',\n",
       " 871: 'tastey',\n",
       " 872: 'soothing',\n",
       " 873: 'drinks',\n",
       " 874: 'cashew',\n",
       " 875: 'garden',\n",
       " 876: 'via',\n",
       " 877: 'belly',\n",
       " 878: 'tummy',\n",
       " 879: 'color',\n",
       " 880: 'biscotti',\n",
       " 881: 'she',\n",
       " 882: 'island',\n",
       " 883: 'choc',\n",
       " 884: 'thats',\n",
       " 885: 'wouldnt',\n",
       " 886: 'pick',\n",
       " 887: 'smoked',\n",
       " 888: 'whats',\n",
       " 889: 'herb',\n",
       " 890: 'links',\n",
       " 891: 'liquid',\n",
       " 892: 'yogurt',\n",
       " 893: 'veggie',\n",
       " 894: 'spiced',\n",
       " 895: 'kick',\n",
       " 896: 'diabetics',\n",
       " 897: 'mixed',\n",
       " 898: 'tart',\n",
       " 899: 'reasonable',\n",
       " 900: 'mouth',\n",
       " 901: 'beat',\n",
       " 902: 'strength',\n",
       " 903: 'maker',\n",
       " 904: 'packs',\n",
       " 905: 'wheres',\n",
       " 906: 'babies',\n",
       " 907: 'night',\n",
       " 908: 'his',\n",
       " 909: 'planet',\n",
       " 910: 'marinade',\n",
       " 911: 'effective',\n",
       " 912: 'meow',\n",
       " 913: 'spaghetti',\n",
       " 914: 'beverage',\n",
       " 915: 'jims',\n",
       " 916: 'creamer',\n",
       " 917: 'single',\n",
       " 918: 'ingredient',\n",
       " 919: 'exceptional',\n",
       " 920: 'cranberry',\n",
       " 921: 'packets',\n",
       " 922: 'flavoring',\n",
       " 923: 'leaves',\n",
       " 924: 'daily',\n",
       " 925: 'second',\n",
       " 926: 'gummies',\n",
       " 927: 'celestial',\n",
       " 928: 'glutino',\n",
       " 929: 'being',\n",
       " 930: 'watery',\n",
       " 931: 'friend',\n",
       " 932: 'smart',\n",
       " 933: 'short',\n",
       " 934: 'cheddar',\n",
       " 935: 'sunflower',\n",
       " 936: 'prefer',\n",
       " 937: 'breast',\n",
       " 938: 'sleep',\n",
       " 939: 'sure',\n",
       " 940: 'animal',\n",
       " 941: 'salsa',\n",
       " 942: 'oolong',\n",
       " 943: 'yeast',\n",
       " 944: 'cakes',\n",
       " 945: 'tassimo',\n",
       " 946: 'diets',\n",
       " 947: 'packaged',\n",
       " 948: 'crisps',\n",
       " 949: 'keep',\n",
       " 950: 'hands',\n",
       " 951: 'know',\n",
       " 952: 'seems',\n",
       " 953: 'seasonings',\n",
       " 954: 'pieces',\n",
       " 955: 'slightly',\n",
       " 956: 'farms',\n",
       " 957: 'anything',\n",
       " 958: 'came',\n",
       " 959: 'their',\n",
       " 960: 'break',\n",
       " 961: 'increase',\n",
       " 962: 'root',\n",
       " 963: 'bonsai',\n",
       " 964: 'her',\n",
       " 965: 'duck',\n",
       " 966: 'theater',\n",
       " 967: 'economical',\n",
       " 968: 'extremely',\n",
       " 969: 'gel',\n",
       " 970: 'lean',\n",
       " 971: 'problem',\n",
       " 972: 'de',\n",
       " 973: 'delivered',\n",
       " 974: 'hint',\n",
       " 975: 'concentrate',\n",
       " 976: 'pricing',\n",
       " 977: 'java',\n",
       " 978: 'fennel',\n",
       " 979: 'shipped',\n",
       " 980: 'rinds',\n",
       " 981: 'saver',\n",
       " 982: 'truffles',\n",
       " 983: 'making',\n",
       " 984: 'mediocre',\n",
       " 985: 'once',\n",
       " 986: 'pay',\n",
       " 987: 'drawer',\n",
       " 988: 'others',\n",
       " 989: 'packing',\n",
       " 990: 'walmart',\n",
       " 991: 'minty',\n",
       " 992: 'holiday',\n",
       " 993: 'per',\n",
       " 994: 'prices',\n",
       " 995: 'damaged',\n",
       " 996: 'van',\n",
       " 997: 'wait',\n",
       " 998: 'subscribe',\n",
       " 999: 'torani',\n",
       " 1000: 'sick',\n",
       " ...}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "product\n",
      "_end_\n"
     ]
    }
   ],
   "source": [
    "while not stop_condition:\n",
    "    #print(1)\n",
    "    preds, st = decoder_model.predict([state_value, body_encoding])\n",
    "\n",
    "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
    "    pred_word_str = vocabulary_inv[pred_idx]\n",
    "    print(pred_word_str)\n",
    "    if pred_word_str == '_end_' or len(decoded_sentence) >= maxlen2:\n",
    "        stop_condition = True\n",
    "        break\n",
    "    decoded_sentence.append(pred_word_str)\n",
    "\n",
    "    # update the decoder for the next word\n",
    "    body_encoding = st\n",
    "    state_value = np.array(pred_idx).reshape(1, 1)\n",
    "    #print(state_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>text_lower</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "      <th>summary_lower</th>\n",
       "      <th>summary_no_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>568439</th>\n",
       "      <td>a-ok</td>\n",
       "      <td>We need this for a recipe my wife is intereste...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>we need this for a recipe my wife is intereste...</td>\n",
       "      <td>we need this for a recipe my wife is intereste...</td>\n",
       "      <td>a-ok</td>\n",
       "      <td>_start_ aok _end_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568442</th>\n",
       "      <td>Great Cafe Latte</td>\n",
       "      <td>This product is great.  Gives you so much ener...</td>\n",
       "      <td>28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>this product is great.  gives you so much ener...</td>\n",
       "      <td>this product is great  gives you so much energ...</td>\n",
       "      <td>great cafe latte</td>\n",
       "      <td>_start_ great cafe latte _end_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568448</th>\n",
       "      <td>Very large ground spice jars.</td>\n",
       "      <td>My only complaint is that there's so much of i...</td>\n",
       "      <td>28</td>\n",
       "      <td>4.0</td>\n",
       "      <td>my only complaint is that there's so much of i...</td>\n",
       "      <td>my only complaint is that theres so much of it...</td>\n",
       "      <td>very large ground spice jars.</td>\n",
       "      <td>_start_ very large ground spice jars _end_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "      <td>25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>great for sesame chicken..this is a good if no...</td>\n",
       "      <td>great for sesame chickenthis is a good if not ...</td>\n",
       "      <td>will not do without</td>\n",
       "      <td>_start_ will not do without _end_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i am very satisfied ,product is as advertised,...</td>\n",
       "      <td>i am very satisfied product is as advertised i...</td>\n",
       "      <td>great honey</td>\n",
       "      <td>_start_ great honey _end_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Summary  \\\n",
       "568439                           a-ok   \n",
       "568442               Great Cafe Latte   \n",
       "568448  Very large ground spice jars.   \n",
       "568449            Will not do without   \n",
       "568453                    Great Honey   \n",
       "\n",
       "                                                     Text  text_length  \\\n",
       "568439  We need this for a recipe my wife is intereste...           22   \n",
       "568442  This product is great.  Gives you so much ener...           28   \n",
       "568448  My only complaint is that there's so much of i...           28   \n",
       "568449  Great for sesame chicken..this is a good if no...           25   \n",
       "568453  I am very satisfied ,product is as advertised,...           20   \n",
       "\n",
       "        summary_length                                         text_lower  \\\n",
       "568439             0.0  we need this for a recipe my wife is intereste...   \n",
       "568442             2.0  this product is great.  gives you so much ener...   \n",
       "568448             4.0  my only complaint is that there's so much of i...   \n",
       "568449             3.0  great for sesame chicken..this is a good if no...   \n",
       "568453             1.0  i am very satisfied ,product is as advertised,...   \n",
       "\n",
       "                                      text_no_punctuation  \\\n",
       "568439  we need this for a recipe my wife is intereste...   \n",
       "568442  this product is great  gives you so much energ...   \n",
       "568448  my only complaint is that theres so much of it...   \n",
       "568449  great for sesame chickenthis is a good if not ...   \n",
       "568453  i am very satisfied product is as advertised i...   \n",
       "\n",
       "                        summary_lower  \\\n",
       "568439                           a-ok   \n",
       "568442               great cafe latte   \n",
       "568448  very large ground spice jars.   \n",
       "568449            will not do without   \n",
       "568453                    great honey   \n",
       "\n",
       "                            summary_no_punctuation  \n",
       "568439                           _start_ aok _end_  \n",
       "568442              _start_ great cafe latte _end_  \n",
       "568448  _start_ very large ground spice jars _end_  \n",
       "568449           _start_ will not do without _end_  \n",
       "568453                   _start_ great honey _end_  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this product is great  gives you so much energy and tastes great  try this cafe latte and all the other flavors and you will not be disappointed'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_no_punctuation'][568442]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {
    "height": "263px",
    "width": "352px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
